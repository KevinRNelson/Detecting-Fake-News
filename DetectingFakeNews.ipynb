{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DetectingFakeNews.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "EquB_BUpBZWx",
        "KRjt1o3zuUnY",
        "-apvWbeR02Yn",
        "y05K0op2MpUY",
        "QrSAYdLxwRWe",
        "M_lRptqK4jGa",
        "vWuHgHBPhSjB",
        "IpK_LVKt_Gg1",
        "uoWONkpSRkiV"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3kbapwOhC4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "159e2fa4-f359-43e3-ddbf-842d47e47e0a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sklearn as sk\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import gensim\n",
        "import string, time\n",
        "from math import floor, ceil\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras import metrics\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import gensim.downloader as api\n",
        "\n",
        "from scipy.sparse.csr import csr_matrix\n",
        "\n",
        "MODEL_PATH = '/'\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "FAKE, REAL = 0, 1\n",
        "SEED = 145\n",
        "OOV = '<OOV>'\n",
        "PAD_VALUE = '0'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig9nTRppziqi",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIV4bvpBznDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_SIZE = .70\n",
        "VALIDATION_SIZE = .15\n",
        "TEST_SIZE = .15\n",
        "assert TRAINING_SIZE + VALIDATION_SIZE + TEST_SIZE == 1\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "TEXT_MAX_LENGTH = 300\n",
        "TRUNCATING = 'post'\n",
        "PADDING = 'post'\n",
        "\n",
        "WORD_SIZE = 300\n",
        "DOC_SIZE = 300\n",
        "\n",
        "SILENT, PROGRESS_BAR, ONE_LINEPER_EPOCH = 0, 1, 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EquB_BUpBZWx",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Formation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6r_zy59Dao-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_title_and_text(dataset) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - dataset: a Pandas DataFrame containing all of the data\n",
        "\n",
        "    usage:\n",
        "      - dataset = combine_title_and_text(dataset)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with the 'title' and 'text' combined into one column titled 'text'\n",
        "    \"\"\"\n",
        "\n",
        "    assert all([col in dataset.columns for col in ['title', 'text', 'label']])\n",
        "    \n",
        "    data = pd.DataFrame({'text': dataset['title'] + \". \" + dataset['text'],\n",
        "                           'label': dataset['label']})\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pqrwsxdDUFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_dataset(include_title = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - include_title: boolean specifying whether the tile should be included\n",
        "    \n",
        "    usage:\n",
        "      - dataset = download_dataset()\n",
        "      - dataset = download_dataset(include_title = False)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas Dataframe containing the complete set of data\n",
        "          - the columns are:\n",
        "            > text\n",
        "            > label\n",
        "    \"\"\"\n",
        "\n",
        "    # obtain the dataset containing all true articles\n",
        "    dataset_real = pd.read_csv('drive/My Drive/Colab Notebooks/Data/True.csv')\n",
        "    # add corresponding class label\n",
        "    dataset_real['label'] = REAL\n",
        "    # if the title is to be added to the text\n",
        "    if include_title:\n",
        "        # remove all uneccessary columns\n",
        "        dataset_real = dataset_real.drop(columns=[col for col in dataset_real.columns if col not in ['title', 'text', 'label']])\n",
        "        # combine the 'title' and 'text' columns into one column named 'text'\n",
        "        dataset_real = combine_title_and_text(dataset_real)\n",
        "    else:\n",
        "        # remove all uneccessary columns\n",
        "        dataset_real = dataset_real.drop(columns=[col for col in dataset_real.columns if col not in ['text', 'label']])\n",
        "\n",
        "    # obtain the dataset containing all fake articles and add corresponding class label\n",
        "    dataset_fake = pd.read_csv('drive/My Drive/Colab Notebooks/Data/Fake.csv')\n",
        "    # add corresponding class label\n",
        "    dataset_fake['label'] = FAKE\n",
        "    # if the title is to be added to the text\n",
        "    if include_title:\n",
        "        # remove all uneccessary columns\n",
        "        dataset_fake = dataset_fake.drop(columns=[col for col in dataset_fake.columns if col not in ['title', 'text', 'label']])\n",
        "        # combine the 'title' and 'text' columns into one column named 'text'\n",
        "        dataset_fake = combine_title_and_text(dataset_fake)\n",
        "    else:\n",
        "        # remove all uneccessary columns\n",
        "        dataset_fake = dataset_fake.drop(columns=[col for col in dataset_fake.columns if col not in ['text', 'label']])\n",
        "\n",
        "    # obtain dataset containing all fake articles\n",
        "    dataset_only_fake = pd.read_csv('drive/My Drive/Colab Notebooks/Data/FakeNews.csv')\n",
        "    # remove all articles not written in English\n",
        "    dataset_only_fake = dataset_only_fake.loc[dataset_only_fake.language == 'english']\n",
        "    # add corresponding class label\n",
        "    dataset_only_fake['label'] = FAKE\n",
        "    # if the title is to be added to the text\n",
        "    if include_title:\n",
        "        # remove all uneccessary columns\n",
        "        dataset_only_fake = dataset_only_fake.drop(columns=[col for col in dataset_only_fake.columns if col not in ['title', 'text', 'label']])\n",
        "        # combine the 'title' and 'text' columns into one column named 'text'\n",
        "        dataset_only_fake = combine_title_and_text(dataset_only_fake)\n",
        "    else:\n",
        "        # remove all uneccessary columns\n",
        "        dataset_only_fake = dataset_only_fake.drop(columns=[col for col in dataset_only_fake.columns if col not in ['text', 'label']])\n",
        "    \n",
        "    # combine the three datasets and remove all columns with nan values\n",
        "    data = pd.concat([dataset_real, dataset_fake, dataset_only_fake]).dropna().reset_index(drop=True)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D5Ve-X7TGI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(data, training_size=TRAINING_SIZE, validation_size=VALIDATION_SIZE, test_size=TEST_SIZE, stratify=True, random_state=SEED) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: the complete dataset to split into training, (validation), and test sets\n",
        "      - training_size: the percentage of the data to use for training\n",
        "      - validation_size: the percentage of the data to use for the validation set\n",
        "      - test_size: the percentage of the data to use for testing\n",
        "      - stratify: boolean value representing if the training, validation, and test sets should be split in a stratified fashion\n",
        "      - random_seed: the np.random.seed value to use for consistency\n",
        "\n",
        "    usage:\n",
        "      - training_set, validation_set, test_set = split_dataset(dataset)\n",
        "\n",
        "    returns:\n",
        "      - this function returns a dictionary containing three entries, with each entry conataining the values to be used in that set\n",
        "      - keys:\n",
        "          > 'training'\n",
        "          > 'validation'\n",
        "          > 'test'\n",
        "    \"\"\"\n",
        "\n",
        "    # check whether the dataset contains the columns text and label\n",
        "    assert 'text' in data.columns and 'label' in data.columns\n",
        "    # check whether the percentages specified for the training, validation and test set add up to one\n",
        "    assert training_size + validation_size + test_size == 1\n",
        "\n",
        "    class_labels = None\n",
        "    if stratify:\n",
        "        # set a 50/50 split between 1's and 0's\n",
        "        class_labels = [0]*ceil(len(dataset.label)/2) + [1]*floor(len(dataset.label)/2)\n",
        "\n",
        "    # obtain the test set, and values for the training and validation sets\n",
        "    train_data, test_data, train_label, test_label = train_test_split(data.text, data.label, test_size=int(len(data)*test_size), random_state=random_state, stratify=class_labels)\n",
        "    \n",
        "    if stratify:\n",
        "        # set a 50/50 split between 1's and 0's\n",
        "        class_labels = [0]*ceil(len(train_label)/2) + [1]*floor(len(train_label)/2)\n",
        "    # obtain the training and validation sets\n",
        "    train_data, validation_data, train_label, validation_label = train_test_split(train_data, train_label, test_size=int(len(data)*validation_size), random_state=random_state, stratify=class_labels)\n",
        "\n",
        "    # return a pandas DataFrame of each of the sets\n",
        "    return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
        "    \"\"\"\n",
        "    return (pd.DataFrame(data={'text': train_data, 'label': train_label}).reset_index(drop=True),\n",
        "            pd.DataFrame(data={'text': validation_data, 'label': validation_label}).reset_index(drop=True),\n",
        "            pd.DataFrame(data={'text': test_data, 'label': test_label}).reset_index(drop=True))\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRjt1o3zuUnY",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws-cj820vksI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(data, print_time=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the text\n",
        "      - print_time: boolean specifying whether or not the amount of time the function took should be printed\n",
        "\n",
        "    usage:\n",
        "      - dataset = remove_punctuation(data)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with all punctuation removed\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # obtain the start time\n",
        "    if print_time:\n",
        "        start_time = time.time()\n",
        "\n",
        "    # removes all forms of punctuation from the dataset\n",
        "    data = data.str.replace('[{}]'.format(string.punctuation + '’‘“”…—–•'), '')\n",
        "\n",
        "\n",
        "    # print the amount of time it took\n",
        "    if print_time:\n",
        "        print(f'it took {round(time.time() - start_time, 2)} seconds to remove puctuation')\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhVyFm9N1h3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(data, print_time=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the text\n",
        "      - print_time: boolean specifying whether or not the amount of time the function took should be printed\n",
        "\n",
        "    usage:\n",
        "      - dataset = remove_stopwords(data)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with all stopwords removed\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # obtain the start time\n",
        "    if print_time:\n",
        "        start_time = time.time()\n",
        "\n",
        "    # removes all english stop words from the dataset\n",
        "    data = data.str.replace(r'\\b(?:{})\\b'.format('|'.join(STOP_WORDS)), '')\n",
        "\n",
        "    # print the amount of time it took\n",
        "    if print_time:\n",
        "        print(f'it took {round(time.time() - start_time, 2)} seconds to remove stopwords')\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUEgd5ln3ob5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_uppercase(data, print_time=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the text\n",
        "      - print_time: boolean specifying whether or not the amount of time the function took should be printed\n",
        "\n",
        "    usage:\n",
        "      - dataset = remove_uppercase(data)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with all text being lowercase\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # obtain the start time\n",
        "    if print_time:\n",
        "        start_time = time.time()\n",
        "\n",
        "    # converts all uppercased letters/words to lowercase\n",
        "    data = data.apply(lambda x: x.lower())\n",
        "   \n",
        "    # print the amount of time it took\n",
        "    if print_time:\n",
        "        print(f'it took {round(time.time() - start_time, 2)} seconds to remove uppercases')\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JABf0yRHNrIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(data, token_length='word') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the text\n",
        "      - token_length: string value representing if the tokenization should split articles into lists of words or sentences\n",
        "          - possible values:\n",
        "            > 'word'\n",
        "            > 'sentence'\n",
        "\n",
        "    usage:\n",
        "      - dataset = tokenize_text(dataset)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with all text tokenized, i.e. each article is a list of the words from that article\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "    assert token_length in ['word', 'sentence']\n",
        "\n",
        "    if token_length == 'word':\n",
        "        # convert each article to a list of the words in the article\n",
        "        data = data.apply(lambda x: word_tokenize(x))\n",
        "    elif token_length == 'sentence':\n",
        "        # convert each article to a list of the sentences in the article\n",
        "        data = data.apply(lambda x: sent_tokenize(x))\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxA4XPZsr02x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_text(data) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the data\n",
        "\n",
        "    usage:\n",
        "      - dataset = join_text(dataset)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with all text converted to a string\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "    assert all([isinstance(text, list) for text in data])\n",
        "\n",
        "    # combine each article into one large string\n",
        "    data = data.apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__B0jxrCTCTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_nonstems(data, print_time=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the text\n",
        "      - print_time: boolean specifying whether or not the amount of time the function took should be printed\n",
        "\n",
        "    usage:\n",
        "      - dataset = remove_nonstems(data)\n",
        "\n",
        "    returns:\n",
        "      - a Pandas DataFrame with all words truncating to just their root stems\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # obtain the start time\n",
        "    if print_time:\n",
        "        start_time = time.time()\n",
        "    \n",
        "    # split each article into a list of the words in the article\n",
        "    data = tokenize_text(data)\n",
        "\n",
        "    # truncate each word into its root steem\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    data = data.apply(lambda x: [porter_stemmer.stem(word) for word in x])\n",
        "\n",
        "    # combine the articles back into a string\n",
        "    data = join_text(data)\n",
        "\n",
        "    # print the amount of time it took\n",
        "    if print_time:\n",
        "        print(f'it took {round(time.time() - start_time, 2)} seconds to remove nonstems')\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL6MhDvrvtHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def clean_text(data, punctuation=True, stopwords=True, uppercase=True, stemming=True, print_time=False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: a Pandas Series containing all of the text\n",
        "      - punctuation: boolean value specifying whether or not to remove all punctuation from the text\n",
        "      - stopwords: boolean value specifying whether or not to remove all stopwords from the text\n",
        "      - uppercase: boolean value specifying whether or not to change all text to lowercase\n",
        "      - stemming: boolean value specifying whether or not to truncate all words in the text to their root wor\n",
        "      - print_time: boolean specifying whether or not the function run time should be printed\n",
        "\n",
        "    usage:\n",
        "      - dataset = clean_text(dataset, clean_time=True)\n",
        "\n",
        "    returns\n",
        "      - the inputed data with all of the specified cleaning functions applied\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    run_clean_text_functions = [punctuation,               stopwords,        uppercase,        stemming]\n",
        "    clean_text_functions     = [remove_punctuation, remove_stopwords, remove_uppercase, remove_nonstems]\n",
        "    clean_text_descriptions  = ['punctuation',           'stopwords',     'uppercases',      'nonstems']\n",
        "\n",
        "    for run, function, description in zip(run_clean_text_functions, clean_text_functions, clean_text_descriptions):\n",
        "        if run:\n",
        "            if print_time:\n",
        "                start_time = time.time()\n",
        "            \n",
        "            data = function(data)\n",
        "\n",
        "            if print_time:\n",
        "                print(f'it took {round(time.time() - start_time, 2)} seconds to remove {description}')\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQsYH98TIIqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_labels(labels) -> pd.Series:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - labels: a pandas series of class labels\n",
        "\n",
        "    usage:\n",
        "      - training_set.label = encode_labels(training_set.labels)\n",
        "\n",
        "    returns:\n",
        "      - the labels encoded between the values 0 and num_of_classes - 1\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(labels, pd.Series)\n",
        "\n",
        "    return preprocessing.LabelEncoder().fit_transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFC504Yy03R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def obtain_vocabulary(training_data, num_words=VOCAB_SIZE, oov_token=OOV) -> Tokenizer:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - training_data: the training articles to create the vocabulary from\n",
        "      - num_words: the size of the vocabulary\n",
        "      - oov_token: string value to use for words not in the vocabulary, i.e. Out Of Vocabulary\n",
        "\n",
        "    usage:\n",
        "      - vocabulary = obtain_vocabulary(training_set)\n",
        "\n",
        "    returns:\n",
        "      - a tokenizer object fit on the training data\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(training_data, pd.Series)\n",
        "\n",
        "    # create a vocaulary for the training data and store the count of each word\n",
        "    vocab = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "    vocab.fit_on_texts(training_data)\n",
        "\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmWa7hoTf4Ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def truncate_vocabulary(vocab, vocab_size=VOCAB_SIZE) -> dict:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - vocab: a dictionary of the entire vocabulary fitted off of the training data\n",
        "      - vocab_size: the number of words allowed in the dictionary\n",
        "\n",
        "    usage:\n",
        "      - vocabulary.word_index = truncate_vocabulary(vocabulary.word_index)\n",
        "\n",
        "    returns:\n",
        "      - a dictionary of words in the vocabulary with size equal to vocab_size\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(vocab, dict)\n",
        "\n",
        "    # remove the padding string from the vocab\n",
        "    del vocab[PAD_VALUE]\n",
        "    # turn the vocab into an interator of its words\n",
        "    vocab = iter(vocab.keys())\n",
        "\n",
        "    # create a new dictionary and store the padding string at index 0\n",
        "    truncated_vocab = dict()\n",
        "    truncated_vocab[PAD_VALUE] = 0\n",
        "    \n",
        "    # cycle through the first vocab_size words in the vocabulary add it to the truncated vocabulary\n",
        "    num_of_words = 1\n",
        "    while num_of_words < vocab_size:\n",
        "        truncated_vocab[next(vocab)] = num_of_words\n",
        "        num_of_words += 1\n",
        "\n",
        "    return truncated_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbH4RgJI02ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(data, vocab, max_length=TEXT_MAX_LENGTH, dtype=object, padding=PADDING, truncating=TRUNCATING, value=PAD_VALUE) -> pd.Series:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: the text data to preprocess\n",
        "      - vocab: the complete vocabulary dictionary\n",
        "      - max_length: the max length of the vocabulary\n",
        "      - dtype: the type of the output sequences\n",
        "          - possible dtype values:\n",
        "            > int32\n",
        "            > object\n",
        "      - padding: string value representing whether to pad before or after each text sequence\n",
        "          - possible padding values:\n",
        "            > 'pre'\n",
        "            > 'post'\n",
        "      - truncating: string value representing whether to remove words before or after each text sequence if the sequence is longer than max_length\n",
        "          - possible truncating values:\n",
        "            > 'pre'\n",
        "            > 'post'\n",
        "      - value: the value to be used for padding\n",
        "\n",
        "    usage:\n",
        "      - training_set.text = preprocess_text(training_set.text, vocabulary)\n",
        "\n",
        "    returns:\n",
        "      - a pandas series where each index correpsonds to a text sequence limited to length max_length,\n",
        "        and all words not in the vocabulary are replaced with the oov token\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # convert each text sequence to its corresponding vocabulary mapping\n",
        "    data = vocab.texts_to_sequences(data)\n",
        "    # truncate or pad each sequence to the max length\n",
        "    data = pd.Series(list(pad_sequences(data, maxlen=max_length, dtype=dtype, padding=padding, truncating=truncating, value=value)))\n",
        "\n",
        "    # dictionary where each key, value pair is the reverse of all key, value pairs in the vocabulary\n",
        "    reverse_vocab = dict([(value, key) for (key, value) in vocab.word_index.items()])\n",
        "\n",
        "    return data.apply(lambda x: ' '.join([reverse_vocab.get(word, '?') for word in x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOy47r8LuROV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_max_normalization(data, lower_bound=0, upper_bound=1):\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - attribute: the column to perform min-max normalization on\n",
        "      - lower_bound: the lower bound of the range to rescale to\n",
        "      - upper_bound: the upper bound of the range to rescale to\n",
        "\n",
        "    returns:\n",
        "      - a pandas series containing the normalized data\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # calculates the minimum and maximum values of the attribute\n",
        "    min_value = min([min(x) for x in data])\n",
        "    max_value = max([max(x) for x in data])\n",
        "\n",
        "    return data.apply(lambda x: lower_bound + ((x - min_value) * (upper_bound - lower_bound))/(max_value - min_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-apvWbeR02Yn",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1N4JjwvwH-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_count_vectorizer(vocabulary, decode_error='strict', analyzer='word', max_df=1, min_df=1, max_features=VOCAB_SIZE) -> sk.feature_extraction.text.CountVectorizer:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - vocabulary: a predetermined vocabulary       \n",
        "      - decode_error: determines what to do if a byte sequence is found that is not of 'utf-8'\n",
        "          - possible decode_errors:\n",
        "            > 'strict' - throw UnicodeDecodeError\n",
        "            > 'ignore'\n",
        "            > 'replace'\n",
        "      - ngram_range: lower and upper bounds for the ngram\n",
        "          - possible ngram_range values:\n",
        "            > (min_n, max_n), i.e. (1, 1) is only unigrams, (1, 2) is unigrams and bigrams\n",
        "      - analyazer: determines what the features should be created from\n",
        "          - possible analyzers:\n",
        "            > 'word'\n",
        "            > 'char'\n",
        "            > 'char_wb'\n",
        "      - max_df: ignore terms with a document frequency stricly higher than the given threshold\n",
        "          - possible max_df values:\n",
        "            > float in range [0, 1]\n",
        "            > int\n",
        "      - min_df: ignore terms with a document frequency stricly lower than the given threshold\n",
        "          - possible min_df values:\n",
        "            > float in range [0, 1]\n",
        "            > int\n",
        "      - max_features: the max size of the vocabulary\n",
        "\n",
        "    usage:\n",
        "      - vectorizer = get_count_vectorizer(dataset.text)\n",
        "\n",
        "    returns: \n",
        "      - an sklearn count vectorizer with the specified parameters\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(vocabulary, dict)\n",
        "\n",
        "    return CountVectorizer(decode_error=decode_error, analyzer=analyzer, max_df=max_df, min_df=min_df, max_features=max_features, vocabulary=vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai1bNoFLKhhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tfidf_vectorizer(data, vocabulary, decode_error='strict', ngram_range=(1, 1), analyzer='word', max_df=1, min_df=1, max_features=VOCAB_SIZE) -> sk.feature_extraction.text.TfidfVectorizer:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: training data to fit the tfidf vectorizer off of\n",
        "      - vocabulary: a predetermined vocabulary       \n",
        "      - decode_error: determines what to do if a byte sequence is found that is not of 'utf-8'\n",
        "          - possible decode_errors:\n",
        "            > 'strict' - throw UnicodeDecodeError\n",
        "            > 'ignore'\n",
        "            > 'replace'\n",
        "      - ngram_range: lower and upper bounds for the ngram\n",
        "          - possible ngram_range values:\n",
        "            > (min_n, max_n), i.e. (1, 1) is only unigrams, (1, 2) is unigrams and bigrams\n",
        "      - analyazer: determines what the features should be created from\n",
        "          - possible analyzers:\n",
        "            > 'word'\n",
        "            > 'char'\n",
        "            > 'char_wb'\n",
        "      - max_df: ignore terms with a document frequency stricly higher than the given threshold\n",
        "          - possible max_df values:\n",
        "            > float in range [0, 1]\n",
        "            > int\n",
        "      - min_df: ignore terms with a document frequency stricly lower than the given threshold\n",
        "          - possible min_df values:\n",
        "            > float in range [0, 1]\n",
        "            > int\n",
        "      - max_features: the max size of the vocabulary\n",
        "\n",
        "    usage:\n",
        "      - tfidf_vectorizer = get_tfidf_vectorizer(dataset.text)\n",
        "\n",
        "    returns:\n",
        "      - an sklearn tfidf vectorizer with the specified parameters\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "    assert isinstance(vocabulary, dict)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(decode_error=decode_error, analyzer=analyzer, max_df=max_df, min_df=min_df, max_features=max_features, vocabulary=vocabulary)\n",
        "    vectorizer.fit(data)\n",
        "\n",
        "    return vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uOwb6Qhx7Th",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word2vec_model(data, min_count=1, size=WORD_SIZE, workers=1) -> gensim.models.word2vec.Word2Vec:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: the training data to fit the model off of\n",
        "      - min_count: the threshold value for words; words greater than this value will be included in the model\n",
        "      - size: the number of dimensions to represent each word\n",
        "      - workers: number of cores to use for parallelization\n",
        "\n",
        "    usage:\n",
        "      - word2vec_model = get_word2vec_model(train_x)\n",
        "\n",
        "    returns:\n",
        "      - a word2vec model trained off of the inputed data\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    return Word2Vec(tokenize_text(data, 'word'), min_count=min_count, size=size, workers=workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mul91OTR3bkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word2vec_embeddings(data, model_word2vec) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: the data to convert to word embeddings\n",
        "      - model_word2vec: the word2vec model to use when converting the text\n",
        "\n",
        "    usage:\n",
        "      - training_data = get_word2vec_embeddings(train_x, model_word2vec)\n",
        "\n",
        "    returns:\n",
        "      - a word level embedding of the inputed text\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    # split all text into a tokenized list of words\n",
        "    data = tokenize_text(data, 'word')\n",
        "\n",
        "    return data.apply(lambda x: model_word2vec.wv[x][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc9Nd_y-geyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_doc2vec_model(data, min_count=1, vector_size=DOC_SIZE, workers=1) -> gensim.models.word2vec.Word2Vec:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: the training data to fit the model off of\n",
        "      - min_count: the threshold value for words; words greater than this value will be included in the model\n",
        "      - size: the number of dimensions to represent each word\n",
        "      - workers: number of cores to use for parallelization\n",
        "\n",
        "    usage:\n",
        "      - word2vec_model = get_word2vec_model(train_x)\n",
        "\n",
        "    returns:\n",
        "      - a word2vec model trained off of the inputed data\n",
        "    \"\"\"\n",
        "\n",
        "    #assert isinstance(data, pd.Series)\n",
        "\n",
        "    return Doc2Vec(data, min_count=min_count, vector_size=vector_size, workers=workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxyG1b3qToZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_doc2vec_embeddings(data, model_doc2vec) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    data = tokenize_text(data, 'word')\n",
        "\n",
        "    return data.apply(lambda x: model_doc2vec.infer_vector(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAxAIOGMO_Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tag_text(data, labels=[]) -> list:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data, pd.Series)\n",
        "\n",
        "    if not len(labels):\n",
        "        labels = data.index\n",
        "\n",
        "    tagged_data = []\n",
        "\n",
        "    for text, label in zip(data, labels):\n",
        "        tagged_data.append(TaggedDocument(words=text.split(), tags=[label]))\n",
        "        \n",
        "    return tagged_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y05K0op2MpUY",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNCnZiWbMoo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, test_data, test_labels):\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - model: the desired model to evaluate\n",
        "      - test_data: the test set to evaluate\n",
        "      - test_labels: the corresponding class labels of the test set\n",
        "\n",
        "    usage:\n",
        "      - evaluate_model(model)\n",
        "\n",
        "    returns:\n",
        "      - loss: the loss of the model\n",
        "      - accuracy: the accuracy of the model\n",
        "    \"\"\"\n",
        "\n",
        "    loss, accuracy = model.evaluate(x=test_data, y=test_labels)\n",
        "\n",
        "    yhat_classes = model.predict_classes(test_data, verbose=0)\n",
        "    # reduce to 1d array\n",
        "    yhat_classes = yhat_classes[:, 0]\n",
        "    \n",
        "    # precision tp / (tp + fp)\n",
        "    precision = precision_score(test_labels, yhat_classes, zero_division=1)\n",
        "    recall = recall_score(test_labels, yhat_classes, zero_division=1)\n",
        "    fscore_weighted = weighted_fscore(.5, precision, recall)\n",
        "\n",
        "    print('Loss: %f' % loss)\n",
        "    print('Accuracy: %f' % accuracy)\n",
        "    print('Precision: %f' % precision)\n",
        "    print('Recall: %f' % recall)\n",
        "    print('fscore: %f' % fscore_weighted)\n",
        "\n",
        "    return loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr5-oiReWQI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_fscore(weight, precision, recall) -> float:\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - weight: integer value representing the weight to give precision\n",
        "      - precision: integer, the precision of the model\n",
        "      - recall: integer, the recall of the model\n",
        "\n",
        "    usage:\n",
        "      - fscore_weighted = weighted_fscore(.5, precision, recall)\n",
        "\n",
        "    returns:\n",
        "      - the calculated weighted f_score of the model\n",
        "    \"\"\"\n",
        "\n",
        "    return (1 + weight**2)*((precision*recall)/((precision*weight**2)+recall))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrSAYdLxwRWe",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Preperation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEORAbPDdOC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain the raw dataset\n",
        "dataset = download_dataset()\n",
        "\n",
        "# clean the dataset\n",
        "dataset.text = clean_text(dataset.text, punctuation=False, stopwords=False, uppercase=False, stemming=False, print_time=True)\n",
        "\n",
        "# remove all duplicate rows\n",
        "#dataset = dataset.drop_duplicates(subset=['text'])\n",
        "\n",
        "# encode the labels\n",
        "dataset.label = encode_labels(dataset.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T-D9Ms9pFQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a training, validation and test set\n",
        "train_x, train_y, validation_x, validation_y, test_x, test_y = split_dataset(dataset, stratify=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crdS044Di5mK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the vocabulary \n",
        "vocabulary = obtain_vocabulary(train_x, num_words=VOCAB_SIZE)\n",
        "# remove all words not in the vocabulary\n",
        "vocabulary.word_index = truncate_vocabulary(vocabulary.word_index, vocab_size=VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SauePyQTSpSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# truncate or pad each article to a certain length and replace each word in the vocabulary with the OOV token\n",
        "train_x      = preprocess_text(     train_x, vocabulary, max_length=TEXT_MAX_LENGTH, padding=PADDING, truncating=TRUNCATING)\n",
        "validation_x = preprocess_text(validation_x, vocabulary, max_length=TEXT_MAX_LENGTH, padding=PADDING, truncating=TRUNCATING)\n",
        "test_x       = preprocess_text(      test_x, vocabulary, max_length=TEXT_MAX_LENGTH, padding=PADDING, truncating=TRUNCATING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_lRptqK4jGa",
        "colab_type": "text"
      },
      "source": [
        "# Count Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBwCaMnptr0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a count vetorizer to count occurences of each word in the vocabulary\n",
        "count_vectorizer = get_count_vectorizer(vocabulary.word_index)\n",
        "count_vectorizer.stop_words_ = None\n",
        "\n",
        "# convert the training, validation, and test sets into count variants\n",
        "count_training_data   = count_vectorizer.transform(train_x).toarray()\n",
        "count_validation_data = count_vectorizer.transform(validation_x).toarray()\n",
        "count_test_data       = count_vectorizer.transform(test_x).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA-tO-bz4mZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model parameters\n",
        "INPUT_SIZE = len(count_training_data[0])\n",
        "EMBED_SIZE = 128\n",
        "DROPOUT_RATE = .5\n",
        "CONSTRAINT = None\n",
        "\n",
        "EPOCHS = 5\n",
        "STEPS_PER_EPOCH = 15\n",
        "SIZE = 2500\n",
        "\n",
        "# parameters used for naming the model\n",
        "MODEL_TYPE = 'BiLSTM'\n",
        "FEATURE_TYPE = 'Count'\n",
        "\n",
        "# if the models performance is higher than this accuracy --> save it\n",
        "ACCEPTABLE_ACCURACY = .70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig2MAxTH5Cn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "count_model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(INPUT_SIZE, EMBED_SIZE,\n",
        "                           embeddings_constraint=CONSTRAINT,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.Dropout(DROPOUT_RATE),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(EMBED_SIZE)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "count_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peZLXy1z4ubc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dictionary to store the models training, and validation set accuracy and loss after each epoch\n",
        "count_model = {[('train_loss', []), ('train_accuracy', []), ('val_loss', []), ('val_accuracy', [])]}\n",
        "\n",
        "# iterate through each epoch\n",
        "for EPOCH in range(1, EPOCHS):\n",
        "    # establish the lower and upper bounds of the batch size\n",
        "    LOWER_BOUND, UPPER_BOUND = 0, min(BATCH_SIZE, len(count_training_data))\n",
        "\n",
        "    # cycle through the batches\n",
        "    while LOWER_BOUND < len(count_training_data):\n",
        "        # fit the model to the current batch\n",
        "        loss, accuracy = count_model.fit( x=count_training_data[LOWER_BOUND:UPPER_BOUND],\n",
        "                                          y=train_y[LOWER_BOUND:UPPER_BOUND],\n",
        "                                          epochs=1,\n",
        "                                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                          verbose=PROGRESS_BAR )\n",
        "        # add the training loss and accuracy to the cache\n",
        "        count_cache['train_loss'].append(loss)\n",
        "        count_cache['train_accuracy'].append(accuracy)\n",
        "      \n",
        "        # update the batch bounds\n",
        "        LOWER_BOUND, UPPER_BOUND = UPPER_BOUND, UPPER_BOUND + BATCH_SIZE\n",
        "\n",
        "    # obtain the loss and accuracy of the model on the validation set\n",
        "    loss, accuracy = count_model.evaluate(x=count_validation_data,\n",
        "                                          y=validation_y)\n",
        "    \n",
        "    count_cache['val_loss'].append(loss)\n",
        "    count_cache['val_accuracy'].append(accuracy)\n",
        "\n",
        "    # save the model if it performed higher than ACCEPTABLE_ACCURACY\n",
        "    if accuracy >= ACCEPTABLE_ACCURACY:\n",
        "        MODEL_NAME = MODEL_TYPE + FEATURE_TYPE + 'ModelEpoch' + str(EPOCH) + '.h5'\n",
        "        count_model.save_weights(filepath=MODEL_PATH + MODEL_NAME, save_format='h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnmexxfLL9qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model on the test set\n",
        "count_model.evaluate(x=count_test_data, y=train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDLYVS6Rfy3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain the performance of the model\n",
        "evaluate_model(count_model, count_test_data, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWuHgHBPhSjB",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hTO4pz0hXyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a tfidf vectorizer to find the term frequency multiplied by the inverse of the document frequency of all words in the articles\n",
        "tfidf_vectorizer = get_tfidf_vectorizer(train_x, vocabulary.word_index)\n",
        "tfidf_vectorizer.stop_words = None\n",
        "\n",
        "# convert the training, validation, and test set into tf-idf variants\n",
        "tfidf_training_data   = tfidf_vectorizer.transform(train_x).toarray()\n",
        "tfidf_validation_data = tfidf_vectorizer.transform(validation_x).toarray()\n",
        "tfidf_test_data       = tfidf_vectorizer.transform(test_x).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dTW_bZ4he1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the model parameters\n",
        "INPUT_SIZE = len(tfidf_training_data[0])\n",
        "EMBED_SIZE = 128\n",
        "DROPOUT_RATE = .5\n",
        "CONSTRAINT = None\n",
        "\n",
        "EPOCHS = 4\n",
        "STEPS_PER_EPOCH = 8\n",
        "BATCH_SIZE = 1250\n",
        "\n",
        "# parameters used for naming the model\n",
        "MODEL_TYPE = 'BiLSTM'\n",
        "FEATURE_TYPE = 'Tfidf'\n",
        "\n",
        "# if the models performance is higher than this accuracy --> save it\n",
        "ACCEPTABLE_ACCURACY = .70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14DsqiuShhOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "tfidf_model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(INPUT_SIZE, EMBED_SIZE,\n",
        "                           embeddings_constraint=CONSTRAINT,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.Dropout(DROPOUT_RATE),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(EMBED_SIZE)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "tfidf_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-kArgQthzqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dictionary to store the models training, and validation set accuracy and loss after each epoch\n",
        "tfidf_cache = {[('train_loss', []), ('train_accuracy', []), ('val_loss', []), ('val_accuracy', [])]}\n",
        "\n",
        "# iterate through each epoch\n",
        "for EPOCH in range(1, EPOCHS):\n",
        "    # establish the lower and upper bounds of the batch size\n",
        "    LOWER_BOUND, UPPER_BOUND = 0, min(BATCH_SIZE, len(tfidf_training_data))\n",
        "\n",
        "    # cycle through the batches\n",
        "    while LOWER_BOUND < len(tfidf_training_data):\n",
        "        # fit the model to the current batch\n",
        "        loss, accuracy = tfidf_model.fit( x=tfidf_training_data[LOWER_BOUND:UPPER_BOUND],\n",
        "                                          y=train_y[LOWER_BOUND:UPPER_BOUND],\n",
        "                                          epochs=1,\n",
        "                                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                          verbose=PROGRESS_BAR ))\n",
        "        # add the training loss and accuracy to the cache\n",
        "        tfidf_cache['train_loss'].append(loss)\n",
        "        tfidf_cache['train_accuracy'].append(accuracy)\n",
        "      \n",
        "        # update the batch bounds\n",
        "        LOWER_BOUND, UPPER_BOUND = UPPER_BOUND, UPPER_BOUND + BATCH_SIZE\n",
        "\n",
        "    # obtain the loss and accuracy of the model on the validation set\n",
        "    loss, accuracy = tfidf_model.evaluate(x=tfidf_validation_data,\n",
        "                                          y=validation_y)\n",
        "\n",
        "    # add the validation loss and accuracy to the cache\n",
        "    tfidf_cache['val_loss'].append(loss)\n",
        "    tfidf_cache['val_accuracy'].append(accuracy)\n",
        "\n",
        "    # save the model if it performed higher than ACCEPTABLE_ACCURACY\n",
        "    if accuracy >= ACCEPTABLE_ACCURACY:\n",
        "        MODEL_NAME = MODEL_TYPE + FEATURE_TYPE + 'ModelEpoch' + str(EPOCH) + '.h5' \n",
        "        tfidf_model.save_weights(filepath=MODEL_PATH + MODEL_NAME, save_format='h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu4B2k_V1J1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model on the test set\n",
        "x = tfidf_model.evaluate(x=tfidf_test_data, y=test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fWFOH591Olb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain the performance of the model\n",
        "evaluate_model(tfidf_model, tfidf_test_data, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kNo2gjt5fEb",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRQweW7V5sEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defines the embedding size\n",
        "WORD2VEC_SIZE = 150\n",
        "\n",
        "# create the Word2Vec model\n",
        "word2vec_model = get_word2vec_model(train_x, size=WORD2VEC_SIZE)\n",
        "\n",
        "# transform the training, validation, and test set into theior corresponding Word2Vec embeddings\n",
        "word2vec_training_data   = get_word2vec_embeddings(train_x, word2vec_model)\n",
        "word2vec_validation_data = get_word2vec_embeddings(validation_x, word2vec_model)\n",
        "word2vec_test_data       = get_word2vec_embeddings(test_x, word2vec_model)\n",
        "\n",
        "# obtain the minimum and maximum values from the training set\n",
        "max_num = max([max(x) for x in word2vec_training_data])\n",
        "min_num = min([min(x) for x in word2vec_training_data])\n",
        "upper_bound = max_num + abs(min_num)\n",
        "\n",
        "# normalize the data sets to be in the range 0 -> upper_bound\n",
        "word2vec_training_data   = np.asarray(list(min_max_normalization(word2vec_training_data, 0, upper_bound)))\n",
        "word2vec_validation_data = np.asarray(list(min_max_normalization(word2vec_validation_data, 0, upper_bound)))\n",
        "word2vec_test_data       = np.asarray(list(min_max_normalization(word2vec_test_data, 0, upper_bound)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ2suXhy6nmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the parameters of the model\n",
        "INPUT_SIZE = len(word2vec_training_data[0])\n",
        "EMBED_SIZE = 100\n",
        "DROPOUT_RATE = .5\n",
        "CONSTRAINT = None\n",
        "\n",
        "EPOCHS = 30\n",
        "STEPS_PER_EPOCH = 50\n",
        "BATCH_SIZE = 2500\n",
        "\n",
        "# parameters used for naming the model\n",
        "MODEL_TYPE = 'BiLSTM'\n",
        "FEATURE_TYPE = 'Word2Vec'\n",
        "\n",
        "# if the model is higher than this accuracy --> save it\n",
        "ACCEPTABLE_ACCURACY = .70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvMF1BsU6-H4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "word2vec_model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(INPUT_SIZE, EMBED_SIZE,\n",
        "                           embeddings_constraint=CONSTRAINT,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.Dropout(DROPOUT_RATE),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(EMBED_SIZE)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "word2vec_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUTd9WuP7vx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dictionary to store the models training, and validation set accuracy and loss after each epoch\n",
        "word2vec_cache = {[('train_loss', []), ('train_accuracy', []), ('val_loss', []), ('val_accuracy', [])]}\n",
        "\n",
        "# iterate through each epoch\n",
        "for EPOCH in range(1, EPOCHS):\n",
        "    # establish the lower and upper bounds of the batch size\n",
        "    LOWER_BOUND, UPPER_BOUND = 0, min(BATCH_SIZE, len(word2vec_training_data))\n",
        "\n",
        "    # cycle through the batches\n",
        "    while LOWER_BOUND < len(training_data):\n",
        "        # fit the model to the current batch\n",
        "        loss, accuracy = word2vec_model.fit( x=word2vec_training_data[LOWER_BOUND:UPPER_BOUND],\n",
        "                                             y=train_y[LOWER_BOUND:UPPER_BOUND],\n",
        "                                             epochs=1,\n",
        "                                             steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                             verbose=VERBOSE )\n",
        "        # add the training loss and accuracy to the cache\n",
        "        word2vec_cache['train_loss'].append(loss)\n",
        "        word2vec_cache['train_accuracy'].append(accuracy)\n",
        "        \n",
        "        # update the batch bounds\n",
        "        LOWER_BOUND, UPPER_BOUND = UPPER_BOUND, UPPER_BOUND + BATCH_SIZE\n",
        "\n",
        "    # obtain the loss and accuracy of the model on the validation set\n",
        "    loss, accuracy = model.evaluate(x=word2vec_validation_data,\n",
        "                                    y=validation_y)\n",
        "    \n",
        "    # add the validation loss and accuracy to the cache\n",
        "    word2vec_cache['val_loss'].append(loss)\n",
        "    word2vec_cache['val_accuracy'].append(accuracy)\n",
        "\n",
        "    # save the model if it performed higher than ACCEPTABLE_ACCURACY\n",
        "    if accuracy >= ACCEPTABLE_ACCURACY:\n",
        "        MODEL_NAME = MODEL_TYPE + FEATURE_TYPE + 'ModelEpoch' + str(EPOCH) + '.h5' \n",
        "        model.save_weights(filepath=MODEL_PATH + MODEL_NAME, save_format='h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKpLLYFO9pTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the model on the test set\n",
        "x = word2vec_model.evaluate(x=word2vec_test_data, y=test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6_8qz5D9r35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain the performance of the model\n",
        "evaluate_model(word2vec_model, test_data, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpK_LVKt_Gg1",
        "colab_type": "text"
      },
      "source": [
        "# Doc2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k1gcF4ILMe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5e01b773-1665-4443-b34d-ea8b375b770b"
      },
      "source": [
        "# defines the embedding size\n",
        "DOC2VEC_SIZE = 150\n",
        "\n",
        "# creates the Doc2Vec model\n",
        "doc2vec_model = get_doc2vec_model(tag_text(train_x), vector_size=DOC2VEC_SIZE)\n",
        "\n",
        "# transform the training, validation, and test set into their corresponding Doc2Vecembeddings\n",
        "doc2vec_training_data   = get_doc2vec_embeddings(train_x, doc2vec_model)\n",
        "doc2vec_validation_data = get_doc2vec_embeddings(validation_x, doc2vec_model)\n",
        "doc2vec_test_data       = get_doc2vec_embeddings(test_x, doc2vec_model)\n",
        "\n",
        "# obtain the minimum and maximum values from the training set\n",
        "max_num = max([max(x) for x in doc2vec_training_data])\n",
        "min_num = min([min(x) for x in doc2vec_training_data])\n",
        "upper_bound = max_num + abs(min_num)\n",
        "\n",
        "# normalize the data sets to be in the range 0 -> upper_bound\n",
        "doc2vec_training_data   = np.asarray(list(min_max_normalization(doc2vec_training_data, 0, upper_bound)))\n",
        "doc2vec_validation_data = np.asarray(list(min_max_normalization(doc2vec_validation_data, 0, upper_bound)))\n",
        "doc2vec_test_data       = np.asarray(list(min_max_normalization(doc2vec_test_data, 0, upper_bound)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etHrbeqg_TEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the parameters of the model\n",
        "INPUT_SIZE = len(doc2vec_training_data[0])\n",
        "EMBED_SIZE = 100\n",
        "DROPOUT_RATE = .5\n",
        "CONSTRAINT = None\n",
        "\n",
        "VERBOSE = PROGRESS_BAR\n",
        "\n",
        "EPOCHS = 30\n",
        "STEPS_PER_EPOCH = 50\n",
        "BATCH_SIZE = 2500\n",
        "\n",
        "# parameters used for naming the model\n",
        "MODEL_TYPE = 'BiLSTM'\n",
        "FEATURE_TYPE = 'Doc2Vec' + str(DOC2VEC_SIZE)\n",
        "\n",
        "# if the model is higher than this accuracy --> save it\n",
        "ACCEPTABLE_ACCURACY = .70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ATKbp_9_XrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "doc2vec_model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(INPUT_SIZE, EMBED_SIZE,\n",
        "                           embeddings_constraint=CONSTRAINT,\n",
        "                           mask_zero=True, \n",
        "                           input_shape=[None]),\n",
        "    keras.layers.Dropout(DROPOUT_RATE),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(EMBED_SIZE)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "doc2vec_model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mDCJXs_bjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dictionary to store the models training, and validation set accuracy and loss after each epoch\n",
        "doc2vec_cache = {[('train_loss', []), ('train_accuracy', []), ('val_loss', []), ('val_accuracy', [])]}\n",
        "\n",
        "# iterate through each epoch\n",
        "for EPOCH in range(1, EPOCHS):\n",
        "    # establish the lower and upper bounds of the batch size\n",
        "    LOWER_BOUND, UPPER_BOUND = 0, min(BATCH_SIZE, len(doc2vec_training_data))\n",
        "\n",
        "    # cycle through the batches\n",
        "    while LOWER_BOUND < len(doc2vec_training_data):\n",
        "        \n",
        "        # fit the model to the batch\n",
        "        loss, accuracy = doc2vec_model.fit( x=doc2vec_training_data[LOWER_BOUND:UPPER_BOUND],\n",
        "                                            y=train_y[LOWER_BOUND:UPPER_BOUND],\n",
        "                                            epochs=1,\n",
        "                                            steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                                            verbose=VERBOSE )\n",
        "        # add the training loss and accuracy to the cache\n",
        "        doc2vec_cache['train_loss'].append(loss)\n",
        "        doc2vec_cache['train_accuracy'].append(accuracy)\n",
        "      \n",
        "        # update the batch bounds\n",
        "        LOWER_BOUND, UPPER_BOUND = UPPER_BOUND, UPPER_BOUND + BATCH_SIZE\n",
        "\n",
        "    # obtain the loss and accuracy of the model on the validation set\n",
        "    loss, accuracy = doc2vec_model.evaluate(x=doc2vec_validation_data,\n",
        "                                            y=validation_y)\n",
        "    \n",
        "    # add the validation loss and accuracy to the cache\n",
        "    doc2vec_cache['val_loss'].append(loss)\n",
        "    doc2vec_cache['val_accuracy'].append(accuracy)\n",
        "\n",
        "    # save the model if it performed higher than ACCEPTABLE_ACCURACY\n",
        "    if accuracy >= ACCEPTABLE_ACCURACY:\n",
        "        MODEL_NAME = MODEL_TYPE + FEATURE_TYPE + 'ModelEpoch' + str(EPOCH) + '.h5' #8000 Vocab size, 300 article length\n",
        "        doc2vec_model.save_weights(filepath=MODEL_PATH + MODEL_NAME, save_format='h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_FHrEBuWuWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "682fc496-ffd6-4df2-c8da-88de510ec2af"
      },
      "source": [
        "# evaluate the model on the test set\n",
        "x = doc2vec_model.evaluate(x=doc2vec_test_data, y=test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "238/238 [==============================] - 9s 37ms/step - loss: 0.6566 - accuracy: 0.6098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrOA__RnX7u_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b4b1efd5-7a36-44a8-c75b-7d65b3557542"
      },
      "source": [
        "# obtain the performance of the model\n",
        "evaluate_model(doc2vec_model, test_data, test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.576555\n",
            "Recall: 0.228942\n",
            "fscore: 0.295079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoWONkpSRkiV",
        "colab_type": "text"
      },
      "source": [
        "# Data Visualization Methods\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdaXezXs5f0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_average_article_length(training_data) -> int:\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return sum([len(article) for article in training_data])/len(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WV5gOFs6deC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_boxplot(data):\n",
        "    \"\"\"\n",
        "    parameters:\n",
        "      - data: the complete dataset to use\n",
        "      - columns: the specified attributes to use in the boxplot\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a figure instance\n",
        "    fig = plt.figure(1, figsize=(4,10))\n",
        "    # Create an axes instance\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_xticklabels('length')\n",
        "\n",
        "    # Create the boxplot\n",
        "    bp = ax.boxplot([data], patch_artist=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yr9h-X-dneA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_model_performance(history, metrics, label='accuracy'):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    if not isinstance(metrics, list):\n",
        "       metrics = [metrics]\n",
        "\n",
        "    assert all([metric in history for metric in metrics])\n",
        "\n",
        "    assert label in ['accuracy', 'loss']\n",
        "\n",
        "    for metric in metrics:\n",
        "        plt.plot(history[metric])\n",
        "\n",
        "    plt.title('model ' + label)\n",
        "    plt.ylabel(label)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'][:len(metrics)], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}